---
title: "Clustering: sports analytics"
author: "Statistical Learning, Bachelor in Data Science and Engineering"
date: 'UC3M, 2022'
output:
  html_document: 
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: no
    toc: no
    toc_depth: 1
  pdf_document:
    css: my-theme.css
    theme: cerulean
    highlight: tango
    number_sections: yes
    toc: yes
    toc_depth: 1
editor_options:
  chunk_output_type: console
---


```{r global_options, include=T, echo = F}
knitr::opts_chunk$set(echo = T, warning=FALSE, message=FALSE)
```

```{r, echo=FALSE}
htmltools::img(src = knitr::image_uri(file.path("uc3m.jpg")), 
               alt = 'logo', 
               style = 'position:absolute; top:0; right:0; padding:10px;',
               width="600",
               height="80")
```

# Motivation

**Sports analytics:** collection of relevant, historical, statistics that can provide a competitive advantage to a team or individual. 

In this case study, we will analyze data from NBA 

In basketball, there are typically 5 positions for players: point guard, shooting guard, small forward, power forward, and center

Hence, NBA players should fall into these 5 categories

Some questions to be answered:

  - But is this really the case? NBA players may have skills that exceed their predefined positions.

  - Can we use data performance to discover new groups (alternative position names) based on player skills (how they play)? What's the meaning of these groups?

Based on data of all-time leaders
from here: http://stats.nba.com/alltime-leaders/

```{r}
# delete everything
rm(list=ls()) 

library(tidyverse)
library(GGally) # ggplot2-based visualization of correlations
library(factoextra) # ggplot2-based visualization of pca
library(cluster)
```

## Load and prepare data  

The dataset contains skill performance of the most important NBA players in the history

See the glossary here: http://stats.nba.com/alltime-leaders/

Performance is per game

```{r}
historical_players.df = read.csv(file = "nba.csv", header=T, sep=";")

glimpse(historical_players.df)
```

These are the 1200 best players in the NBA history

## Missing values

```{r}
hist(rowMeans(is.na(historical_players.df)))

barplot(colMeans(is.na(historical_players.df)), las=2)

# skip players with NA information in any variable
historical_players.df <-
  historical_players.df[rowSums(is.na(historical_players.df)) == 0,]
dim(historical_players.df)

```

Conclusions? What can we do?

Create the data frame

```{r}
nba <- historical_players.df[,3:ncol(historical_players.df)] #removing ID and Name
nba <- as.data.frame(sapply(nba, as.numeric))
names = historical_players.df[,2]

dim(nba)
summary(nba)
```

Around 200 players were deleted.

## Some feature engineering

The following is optional, but makes sense for a coach
I.e. we will use variables depending on player's skills, not coache's decisions

```{r}
# skip variable gp (games played) which is decided by the coach
gp = nba$gp
nba$gp = NULL

# skip the min variable (minutes played per game) which is decided by the coach
min = nba$min
nba$min = NULL
```

Take care: there are some redundant variables, for instance

- fgm, fga, and fg_pct
- fg3m, fg3a, and fg3_pct
- ftm, fta, and ft_pct
- oreb, dreb, and reb 
   
Should we skip some of previous variables?

```{r}
nba = nba[,-c(3,6,9,12,19,20,21,22)]
dim(nba)
```

We can also remove variable pts, because pts = fgm+fg3m+ftm 


## Descriptive Analysis

Our input has dimension $p=14$, that implies $2^p$ different relations between the variables.

Remember PCA interpretation and output

```{r}
pca = prcomp(nba, scale=T)

# Plot the first two scores
data.frame(z1=-pca$x[,1],z2=pca$x[,2]) %>% 
  ggplot(aes(z1,z2,label=names)) + geom_point(size=0) +
  labs(title="PCA", x="PC1", y="PC2") +
  theme_bw() +theme(legend.position="bottom") + geom_text(size=2, hjust=0.6, vjust=0, check_overlap = TRUE) 
```

We will use this representation when plotting our clusters. That is, the position of the players will be the same, but we will add colors for different clusters. This will be called the **clusplot**.

# Clustering

## kmeans

Let's start creating 5 clusters (the typical number of different positions) using kmeans

```{r}
# always scale before CLUSTEIRNG
X = scale(nba)

fit = kmeans(X, centers=5, nstart=100)
groups = fit$cluster
groups
```

The output is one category for each player. This is usually the case for all the (deterministic) clustering tools

Are the groups well balanced?

```{r}
barplot(table(groups), col="plum3", main = "CLUSTER GROUPS")
```

Insights?

## Interpretation

Let's understand the meaning of the centers (artificial players representing each group)

```{r}
centers=fit$centers
centers
```

Who are the players in the first group? And in the second one?

```{r}
i=1  # plotting the centers in cluster 1
bar1=barplot(centers[i,], las=2, col="darkblue", ylim=c(-2,2), main=paste("Cluster", i,": Group center in blue, global center in red"))
points(bar1,y=apply(X, 2, quantile, 0.50),col="red",pch=19)
```

The blue bars represent the center of a group, whereas the red points represent the center of all the NBA players

```{r}
i=2  # plotting the centers in cluster 2
bar2=barplot(centers[i,], las=2, col="darkblue", ylim=c(-2,2), main=paste("Cluster", i,": Group center in blue, global center in red")) 
points(bar2,y=apply(X, 2, quantile, 0.50),col="red",pch=19)
```

## The clusplot

```{r}
fviz_cluster(fit, data = X, geom = c("point"),ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+geom_text(label=names,hjust=0, vjust=0,size=2,check_overlap = T)+scale_fill_brewer(palette="Paired")

```

## The Silhouette plot

The silhouette value in [-1,1] measures the similarity (cohesion) of a data point to its cluster relative to other clusters (separation). 

Silhouette plots rely on a distance metric and suggest that the data matches its own cluster well.

The larger the silhouette widths, the better.

```{r}
d <- dist(X, method="euclidean")  
sil = silhouette(groups, d)
plot(sil, col=1:5, main="", border=NA)
summary(sil)
```

## How many groups?

Let's see what happens with two groups

```{r}
## EXECUTE HERE A KMEANS TOOL FOR TWO GROUPS

```

```{r}
centers=fit$centers

i=1  # plottinng the centers in cluster 1
bar1=barplot(centers[i,], las=2, col="darkblue", ylim=c(-2,2), main=paste("Cluster", i,": Group center in blue, global center in red"))
points(bar1,y=apply(X, 2, quantile, 0.50),col="red",pch=19)
```

```{r}
i=2  # plottinng the centers in cluster 2
bar2=barplot(centers[i,], las=2, col="darkblue", ylim=c(-2,2), main=paste("Cluster", i,": Group center in blue, global center in red"))
points(bar2,y=apply(X, 2, quantile, 0.50),col="red",pch=19)
```

Now the interpretation is easier, isn't it?

Clusplot

```{r}
fviz_cluster(fit, data = X, geom = c("point"),ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+geom_text(label=names,hjust=0, vjust=0,size=2,check_overlap = T)+scale_fill_brewer(palette="Paired")
```

Silhouette plot

```{r}
d <- dist(X, method="euclidean")  
sil = silhouette(groups,d)
plot(sil, col=1:2, main="", border=NA)
summary(sil)
```

In terms of Silhouette plot, it seems 2 clusters is better than 5: the average width is higher

Hence, how many clusters?

In theory, we cannot answer this question. In practice, we can have some hints using screeplots:

Based on wss (total within sum of square)

```{r}
# ELBOW METHOD
fviz_nbclust(X, kmeans, method = 'wss')
```

Based on 

```{r}
fviz_nbclust(X, kmeans, method = 'silhouette')
```

Based on the gap statistic (using bootstrap)

```{r}
fviz_nbclust(X, kmeans, method = 'gap_stat', k.max = 20)
```

Insights?

# Profile variables

We have some variables not included in the clustering. These are called **profiles** and may help us to understand better the clusters.

Let's consider 3 clusters

```{r}
fit = kmeans(X, centers=3, nstart=100)
groups = fit$cluster
```

Let's see the distribution of the clusters considering the minutes played

```{r}
as.data.frame(X) %>% mutate(cluster=factor(groups), names=names, min=min, gp=gp) %>%
  ggplot(aes(x = cluster, y = min)) + 
  geom_boxplot(fill="lightblue") +
  labs(title = "Minutes played by cluster", x = "", y = "", col = "") 
```

Seems players in one cluster play less minutes per game. It seems this cluster contains the worse players

Who are the other two clusters?

```{r}
as.data.frame(X) %>% mutate(cluster=factor(groups), names=names, min=min, gp=gp) %>%
  ggplot(aes(x = cluster, y = gp)) + 
  geom_boxplot(fill="lightblue") +
  labs(title = "Games played by cluster", x = "", y = "", col = "") 
```

Similar insights: one cluster has less experience

Clusplot

```{r}
fviz_cluster(fit, data = X, geom = c("point"),ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+geom_text(label=names,hjust=0, vjust=0,size=2,check_overlap = T)+scale_fill_brewer(palette="Paired")
```

The clusplot is clearer: one cluster contains the worst players, and the best players are divided in two: interior and exterior players.

# kmeans with Mahalanobis distance

By default kmeans uses the Euclidean distance: circular distribution not considering correlations between variables

But how to incorporate these correlations, i.e. an elliptical distribution?

The Mahalanobis distance or the multivariate standardization: 

```{r}
S_x <- cov(nba)
iS <- solve(S_x)
e <- eigen(iS)
V <- e$vectors
B <- V %*% diag(sqrt(e$values)) %*% t(V)
Xtil <- scale(nba,scale = FALSE)
nbaS <- Xtil %*% B
```

Be careful: here we are assuming all the clusters have the same covariance matrix, that is, the same shape and orientations as the whole data

If this is not the case, we need mixture models (probabilistic clusters)

kmeans with 3 clusters assuming an elliptic distribution

```{r}
fit.mahalanobis = kmeans(nbaS, centers=3, nstart=100)
groups = fit.mahalanobis$cluster
centers=fit.mahalanobis$centers
colnames(centers)=colnames(X)
centers
```

```{r}
i=1  # plotting the centers in cluster 1
bar1=barplot(centers[i,], las=2, col="darkblue", ylim=c(-2,2), main=paste("Cluster", i,": Group center in blue, global center in red"))
points(bar1,y=apply(X, 2, quantile, 0.50),col="red",pch=19)
```

```{r}
i=2  # plotting the centers in cluster 2
bar2=barplot(centers[i,], las=2, col="darkblue", ylim=c(-2,2), main=paste("Cluster", i,": Group center in blue, global center in red"))
points(bar2,y=apply(X, 2, quantile, 0.50),col="red",pch=19)
```

```{r}
i=3  # plotting the centers in cluster 3
bar3=barplot(centers[i,], las=2, col="darkblue", ylim=c(-2,2), main=paste("Cluster", i,": Group center in blue, global center in red"))
points(bar3,y=apply(X, 2, quantile, 0.50),col="red",pch=19)
```

clusplot

```{r}
fviz_cluster(fit.mahalanobis, data = X, geom = c("point"),ellipse.type = 'norm', pointsize=1)+
  theme_minimal()+geom_text(label=names,hjust=0, vjust=0,size=2,check_overlap = T)+scale_fill_brewer(palette="Paired")
```

Different insights now...

# How similar are the clusters?

The Adjusted Rand index compares two classifications:
the closer to 1 the more agreement

```{r}
library(mclust)
adjustedRandIndex(fit$cluster, fit.mahalanobis$cluster) 
```

Hence, the Euclidean distance gives very different clusters than the Mahalanobis one

Which clustering do you prefer?
